On the off chance that the machine has this capacity, which is utilized in Natural Language Processing (NLP), it will be useful to take care of some sentence generation problems, machine interpretation, programmed question and answer, and other application situations. This paper gives an overview of the information-driven methodology for a concept-to-text generation. Long Short-Term Memory(LSTM), is a neural system model extensively utilized in data processing and predictions.
Most text generation dependent on it can just produce word one way from start word, or it can not keep the keyword in the outcome. Our paper is to explore a way for LSTM to generate the context before and after the keyword. we also built the 1-dimensional Convolution Neural Network (CNN) for generating the text. The resulting system can produce textual output continuously in a sequence by considering linguistic inputs.
